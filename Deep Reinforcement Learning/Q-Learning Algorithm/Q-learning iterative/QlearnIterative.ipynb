{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW8-1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fPn0tdm3d5Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "widgtwPG3x-U",
        "colab_type": "code",
        "outputId": "6f5bb726-90dc-495d-af12-c5082cc0a62e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import os\n",
        "import random\n",
        "import sys\n",
        "import operator\n",
        "import numpy as np\n",
        "!pip install environs"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: environs in /usr/local/lib/python3.6/dist-packages (6.1.0)\n",
            "Requirement already satisfied: marshmallow>=2.7.0 in /usr/local/lib/python3.6/dist-packages (from environs) (3.2.1)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.6/dist-packages (from environs) (0.10.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAGsKBbs3y9j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class QLearningModel:\n",
        "\n",
        "    def __init__(self, col=5, row=5):\n",
        "        self.col = col  \n",
        "        self.row = row  \n",
        "        self.stateProperties = (col, row)\n",
        "        self.actionNum = (4,)\n",
        "        self.action = {\"up\": 0, \"right\": 1, \"down\": 2, \"left\": 3}\n",
        "        self.coordinates = [(-1, 0), (0, 1), (1, 0), (0, -1)]  # translations\n",
        "        # Define the table for rewards\n",
        "        self.R = self.rewardsCollection()  \n",
        "\n",
        "    def Intializer(self):\n",
        "        # Place agent to top-left grid corner\n",
        "        self.state = (0, 0)\n",
        "        return self.state\n",
        "\n",
        "    def move(self, action):\n",
        "       \n",
        "        nextState = (self.state[0] + self.coordinates[action][0], self.state[1] + self.coordinates[action][1])\n",
        "        reward = self.R[self.state + (action,)]\n",
        "        # Terminate if we reach target position\n",
        "        finished = (nextState[0] == self.col - 1) and (nextState[1] == self.row - 1)\n",
        "       \n",
        "        self.state = nextState\n",
        "        return nextState, reward, finished\n",
        "\n",
        "    def actionPossible(self):\n",
        "        actions_allowed = []\n",
        "        y, x = self.state[0], self.state[1]\n",
        "        if (y > 0): \n",
        "            actions_allowed.append(self.action[\"up\"])\n",
        "        if (y < self.col - 1): \n",
        "            actions_allowed.append(self.action[\"down\"])\n",
        "        if (x > 0):  \n",
        "            actions_allowed.append(self.action[\"left\"])\n",
        "        if (x < self.row - 1):  \n",
        "            actions_allowed.append(self.action[\"right\"])\n",
        "        actions_allowed = np.array(actions_allowed, dtype=int)\n",
        "        return actions_allowed\n",
        "\n",
        "    def rewardsCollection(self):\n",
        "        # Define agent rewards R[s,a]\n",
        "        goalRewards = 600  # reward if reach to final state\n",
        "        nonGoalRewards = -5  # penalty \n",
        "        R = nonGoalRewards * np.ones(self.stateProperties + self.actionNum, dtype=float) \n",
        "        R[self.col - 2, self.row - 1, self.action[\"down\"]] = goalRewards  \n",
        "        R[self.col - 1, self.row - 2, self.action[\"right\"]] = goalRewards  \n",
        "        return R"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llwhqPFh31lD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Agent:\n",
        "\n",
        "    def __init__(self, env):\n",
        "        self.stateProperties = env.stateProperties\n",
        "        self.actionNum = env.actionNum\n",
        "        # Agent learning parameters\n",
        "        self.epsilon = 0.04  # probability\n",
        "        self.epsilon_decay = 0.1 \n",
        "        self.beta = 0.1 \n",
        "        self.gamma = 0.7 # Reward discount\n",
        "        # Initialize Q[s,a] table\n",
        "        self.Q = np.zeros(self.stateProperties + self.actionNum, dtype=float)\n",
        "\n",
        "    def get_action(self, env):\n",
        "        \n",
        "        if random.uniform(0, 1) < self.epsilon:\n",
        "            return np.random.choice(env.actionPossible())\n",
        "        else:\n",
        "            # exploit on allowed actions\n",
        "            state = env.state;\n",
        "            actions_allowed = env.actionPossible()\n",
        "            Q_s = self.Q[state[0], state[1], actions_allowed]\n",
        "            greedyActions = actions_allowed[np.flatnonzero(Q_s == np.max(Q_s))]\n",
        "            return np.random.choice(greedyActions)\n",
        "\n",
        "    def train(self, memory):\n",
        "        (state, action, nextState, reward, finished) = memory\n",
        "        sa = state + (action,)\n",
        "        self.Q[sa] += self.beta * (reward + self.gamma * np.max(self.Q[nextState]) - self.Q[sa])\n",
        "\n",
        "    def ViewGreedyPolicy(self):\n",
        "        greedyPolicy = np.zeros((self.stateProperties[0], self.stateProperties[1]), dtype=int)\n",
        "        for x in range(self.stateProperties[0]):\n",
        "            for y in range(self.stateProperties[1]):\n",
        "                greedyPolicy[y, x] = np.argmax(self.Q[y, x, :])\n",
        "        print(\"\\nGreedy policy:\")\n",
        "        print(greedyPolicy)\n",
        "        print()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NciVTedv35mI",
        "colab_type": "code",
        "outputId": "553b4d75-7aa9-4eb8-83e5-e942d7921a68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "env = QLearningModel(col=5, row=5)\n",
        "agent = Agent(env)\n",
        "for item in range(600):\n",
        "    i, reward_i = 0, 0\n",
        "    state = env.Intializer()\n",
        "    while 1:\n",
        "        action = agent.get_action(env)\n",
        "        nextState, reward, finished = env.move(action)\n",
        "        agent.train((state, action, nextState, reward, finished))\n",
        "        i += 1\n",
        "        reward_i += reward\n",
        "        if finished:\n",
        "            break\n",
        "        state = nextState\n",
        "    agent.epsilon = max(agent.epsilon * agent.epsilon_decay, 0.01)\n",
        "\n",
        "    if (item == 599):\n",
        "        agent.ViewGreedyPolicy()\n",
        "        for (key, val) in sorted(env.action.items(), key=operator.itemgetter(1)):\n",
        "            print(\" Action['{}'] = {}\".format(key, val))\n",
        "        print()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Greedy policy:\n",
            "[[2 1 1 1 2]\n",
            " [2 0 1 1 2]\n",
            " [2 3 2 1 2]\n",
            " [2 1 1 2 2]\n",
            " [1 1 1 1 0]]\n",
            "\n",
            " Action['up'] = 0\n",
            " Action['right'] = 1\n",
            " Action['down'] = 2\n",
            " Action['left'] = 3\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UASD1Ns7371_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}